{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c496ae1c-05c0-49cf-97e0-3a87b4debce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4184458778.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[22], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install datasets\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e738f14-33d2-4406-8155-917a457712a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datastructures\n",
      "  Downloading datastructures-0.1.0.tar.gz (88 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: datastructures\n",
      "  Building wheel for datastructures (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[64 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /tmp/pip-install-e2ytoe48/datastructures_1b8c41509b1c45e19839e098227bd662/setup.py:7: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  \u001b[31m   \u001b[0m   import pkg_resources\n",
      "  \u001b[31m   \u001b[0m /home/coco97/miniconda3/envs/extract-info/lib/python3.9/site-packages/setuptools/dist.py:498: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Usage of dash-separated 'license-file' will not be supported in future\n",
      "  \u001b[31m   \u001b[0m         versions. Please use the underscore name 'license_file' instead.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         This deprecation is overdue, please update your project and remove deprecated\n",
      "  \u001b[31m   \u001b[0m         calls to avoid build errors in the future.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   opt = self.warn_dash_deprecation(opt, section)\n",
      "  \u001b[31m   \u001b[0m /home/coco97/miniconda3/envs/extract-info/lib/python3.9/site-packages/setuptools/config/setupcfg.py:293: _DeprecatedConfig: Deprecated config in `setup.cfg`\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         The license_file parameter is deprecated, use license_files instead.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         This deprecation is overdue, please update your project and remove deprecated\n",
      "  \u001b[31m   \u001b[0m         calls to avoid build errors in the future.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   parsed = self.parsers.get(option_name, lambda x: x)(value)\n",
      "  \u001b[31m   \u001b[0m /home/coco97/miniconda3/envs/extract-info/lib/python3.9/site-packages/setuptools/__init__.py:80: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Requirements should be satisfied by a PEP 517 installer.\n",
      "  \u001b[31m   \u001b[0m         If you are using pip, you can try `pip install --use-pep517`.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   dist.fetch_build_eggs(dist.setup_requires)\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/datastructures\n",
      "  \u001b[31m   \u001b[0m copying datastructures/__init__.py -> build/lib.linux-x86_64-cpython-39/datastructures\n",
      "  \u001b[31m   \u001b[0m copying datastructures/helpers.py -> build/lib.linux-x86_64-cpython-39/datastructures\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing datastructures.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to datastructures.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to datastructures.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'datastructures.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'datastructures.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m error: Error: setup script specifies an absolute path:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     /tmp/pip-install-e2ytoe48/datastructures_1b8c41509b1c45e19839e098227bd662/datastructures/structures.c\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m setup() arguments must *always* be /-separated paths relative to the\n",
      "  \u001b[31m   \u001b[0m setup.py directory, *never* absolute paths.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for datastructures\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for datastructures\n",
      "Failed to build datastructures\n",
      "\u001b[31mERROR: Could not build wheels for datastructures, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datastructures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fb4d25c-db5f-48b3-b081-f3cad0aaed03",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datastructures'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearSVC\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultinomialNB\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatastructures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_json, load_json, get_index, split\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m  \u001b[38;5;66;03m# Bibliothèque pour le traitement du langage naturel\u001b[39;00m\n\u001b[1;32m     21\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datastructures'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Apr 28 21:22:22 2024\n",
    "\n",
    "@author: guilhem\n",
    "\"\"\"\n",
    "\n",
    "#__________MODULES\n",
    "import random\n",
    "random.seed(42)\n",
    "import math\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from datastructures import save_json, load_json, get_index, split\n",
    "import nltk  # Bibliothèque pour le traitement du langage naturel\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "#__________FUNCTIONS\n",
    "\n",
    "\n",
    "#__________MODULES\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Ce programme a pour but de tester différentes formes de vectorisation et de models pour la détection de fake-news\")\n",
    "    parser.add_argument(\n",
    "        \"file\",\n",
    "        help=\"Chemin vers le fichier qui contient le corpus au format json.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-p\",\n",
    "        \"--preprocess\",\n",
    "        choices=[\"yes\", \"no\"],\n",
    "        default=\"no\",\n",
    "        help=\"Supprimer les stopwords de l'analyse.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-v\",\n",
    "        \"--vectorize\",\n",
    "        choices=[\"count\", \"tfidf\"],\n",
    "        default=\"count\",\n",
    "        help=\"Choisir le vectorizer -> 'count' : CountVectorizer | 'tfidf' : TfidfVectorizer\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-m\",\n",
    "        \"--model\",\n",
    "        choices=[\"svc\", \"multi\"],\n",
    "        default=\"svc\",\n",
    "        help=\"Choisir un model : 'svc' : LinearSVC | 'multi' MultinomialNB | \"\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.file.split(\".\")[-1] == \"json\":\n",
    "        path_corpura = args.file\n",
    "    else :\n",
    "        print(\"Le fichier contenant les données doit être au format JSON\")\n",
    "        sys.exit()\n",
    "    \n",
    "    # Appel la fonction load_json\n",
    "    dataset = load_json(path_corpura)\n",
    "    \n",
    "    # Appel la fonction get_index\n",
    "    corpus_index = get_index(dataset, \"origin\")\n",
    "    # print(corpus_index)\n",
    "    \n",
    "    # Découpage des données en 2 set : train(90%), test(10%)\n",
    "    id_liste = [item[\"id\"] for item in dataset]\n",
    "    \n",
    "    # Train\n",
    "    size_train = math.ceil(len(id_liste) * 0.9)\n",
    "    train = random.sample(id_liste, size_train)\n",
    "    corpus_train = split(dataset, train, \"../Corpus/train.json\")\n",
    "    train_index = get_index(corpus_train, \"train\")\n",
    "\n",
    "    # Test\n",
    "    test = list(set(id_liste).difference(train))\n",
    "    corpus_test = split(dataset, test, \"../Corpus/test.json\")\n",
    "    test_index = get_index(corpus_test, \"test\")\n",
    "    \n",
    "    # On recupère le contenu textuel de chaque set\n",
    "    train_txt = corpus_train[\"content\"]\n",
    "    test_txt = corpus_test[\"content\"]\n",
    "    \n",
    "    # On recupère le label \"rating\" de chaque set\n",
    "    train_labels = corpus_train[\"rating\"]\n",
    "    test_labels = corpus_test[\"rating\"]\n",
    "    \n",
    "    # Liste de stopwords en français\n",
    "    # french_stop_words = stopwords.words('french')\n",
    "    # count_vectorizer = CountVectorizer(stop_words=french_stop_words)\n",
    "    # tfidf_vectorizer = TfidfVectorizer(stop_words=french_stop_words, max_df=0.7)\n",
    "    \n",
    "    # On vectorise selon le vectorizer choisi\n",
    "    if args.vectorize == \"count\":\n",
    "        vectorizer = CountVectorizer()\n",
    "        X_train = vectorizer.fit_transform(train_txt) # Ajustement sur les données d'entraînement\n",
    "        X_test = vectorizer.transform(test_txt) # Transformation des données de test en utilisant le même vocabulaire\n",
    "    elif args.vectorize == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X_train = vectorizer.fit_transform(train_txt)\n",
    "        X_test = vectorizer.transform(test_txt)\n",
    "        \n",
    "    # Entrainement du modèle sur le Train selon le modèle choisi\n",
    "    if args.model == \"svc\":\n",
    "        clf = LinearSVC().fit(X_train, train_labels)\n",
    "    elif args.model == \"multi\":\n",
    "        clf = MultinomialNB().fit(X_train, train_labels)\n",
    "\n",
    "    #Score\n",
    "    print(\"Score : \", clf.score(X_test, test_labels))\n",
    "    \n",
    "    #Predict\n",
    "    print(\"predictions:\", clf.predict(X_test))\n",
    "    \n",
    "    #Donnee\n",
    "    print(\"vraies classes:\",test_labels)\n",
    "    \n",
    "    pred = clf.predict(X_test)\n",
    "    \n",
    "    # Visualisation des résultats\n",
    "    cm = confusion_matrix(test_labels, pred, labels=clf.classes_)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=clf.classes_).plot()\n",
    "    disp.plot()\n",
    "    \n",
    "    # Sauvegarde de la figure\n",
    "    plt.savefig(f'../Output/{args.vectorize}-{args.model}.png')\n",
    "\n",
    "#__________MAIN\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "###_END_###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5ac8f6-cb1c-40c9-bf17-3ce52273cc32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f69faf4-bd33-4d42-b053-2855e32ee00c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
