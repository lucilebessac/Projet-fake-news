{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51761df8-17a7-4864-9bbe-cde369ee67ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 219\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m#__________MAIN\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m###_END_###\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 156\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m path_corpura \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Data/data.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Appel la fonction load_json\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_corpura\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m corpus_index \u001b[38;5;241m=\u001b[39m get_index(dataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morigin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# print(corpus_index)\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Découpage des données en 3 set : train(80%), test(10%), dev(10%)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 74\u001b[0m, in \u001b[0;36mload_json\u001b[0;34m(path_corpura)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path_corpura, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     73\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m---> 74\u001b[0m corpus \u001b[38;5;241m=\u001b[39m Corpus([Article(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mitem) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m json_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m     75\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([\u001b[38;5;28mvars\u001b[39m(article) \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m corpus\u001b[38;5;241m.\u001b[39mitems])\n\u001b[1;32m     76\u001b[0m data \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(data)\n",
      "Cell \u001b[0;32mIn[4], line 74\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path_corpura, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     73\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m---> 74\u001b[0m corpus \u001b[38;5;241m=\u001b[39m Corpus([\u001b[43mArticle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m json_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m     75\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([\u001b[38;5;28mvars\u001b[39m(article) \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m corpus\u001b[38;5;241m.\u001b[39mitems])\n\u001b[1;32m     76\u001b[0m data \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(data)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'id'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Apr 28 21:22:22 2024\n",
    "\n",
    "@author: guilhem\n",
    "\"\"\"\n",
    "\n",
    "#__________MODULES\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(42)\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from typing import List, Dict\n",
    "from datasets import Dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#__________DATACLASS\n",
    "@dataclass\n",
    "class Article:\n",
    "    id: int\n",
    "    url: str\n",
    "    author: str\n",
    "    date: str\n",
    "    rating: str\n",
    "    title: str\n",
    "    resume: str\n",
    "    content: str\n",
    "    category: str\n",
    "\n",
    "@dataclass\n",
    "class Corpus:\n",
    "    items: list[Article]\n",
    "\n",
    "#__________FUNCTIONS\n",
    "def save_json(corpus: Corpus, path: str) -> None:\n",
    "    \"\"\"\n",
    "    Sauvegarde un objet Corpus au format JSON.\n",
    "\n",
    "    Parameters:\n",
    "    corpus (Corpus): L'objet Corpus à sauvegarder.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    data = {\"items\": []}\n",
    "    for item in corpus.items:\n",
    "        item_dict = asdict(item)\n",
    "        data[\"items\"].append(item_dict)\n",
    "    with open(path, \"w\", encoding=\"utf8\") as file:\n",
    "        json.dump(data, file, indent=2)\n",
    "\n",
    "\n",
    "def load_json(path_corpura: str) -> Dataset : \n",
    "    \"\"\"\n",
    "    Charge un fichier JSON et le convertit en objet Dataset.\n",
    "\n",
    "    Parameters :\n",
    "    path_corpura -- le chemin vers le fichier JSON à charger\n",
    "\n",
    "    Returns :\n",
    "    Un objet Dataset contenant les données du fichier JSON\n",
    "    \"\"\"\n",
    "    with open(path_corpura, \"r\", encoding=\"utf8\") as file:\n",
    "        json_data = json.load(file)\n",
    "    corpus = Corpus([Article(**item) for item in json_data['items']])\n",
    "    data = pd.DataFrame([vars(article) for article in corpus.items])\n",
    "    data = Dataset.from_pandas(data)\n",
    "    return data\n",
    "\n",
    "def get_index(dataset: List[Dict[str, str]], name: str) -> Dict[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Récupère des informations sur les éléments du dataset en fonction de leur note.\n",
    "\n",
    "    Parameters :\n",
    "    dataset -- une liste de dictionnaires représentant les données\n",
    "    name -- une chaîne de caractères représentant le nom de l'ensemble de données\n",
    "\n",
    "    Returns :\n",
    "    Un dictionnaire contenant des listes d'identifiants d'éléments, classées par note\n",
    "    \"\"\"\n",
    "    \n",
    "    liste_true = []\n",
    "    liste_false = []\n",
    "    liste_mix = []\n",
    "    liste_unknow = []\n",
    "    \n",
    "    for item in dataset:\n",
    "        if item[\"rating\"] == \"Vrai\":\n",
    "            liste_true.append(item[\"id\"])\n",
    "        elif item[\"rating\"] == \"Faux\":\n",
    "            liste_false.append(item[\"id\"])\n",
    "        elif item[\"rating\"] == \"Du vrai / du faux\":\n",
    "            liste_mix.append(item[\"id\"])\n",
    "        else: \n",
    "            liste_unknow.append(item[\"id\"])\n",
    "            \n",
    "    dico_info = {\"Vrai\" : liste_true,\n",
    "                 \"Faux\" : liste_false,\n",
    "                 \"Mix\" : liste_mix,\n",
    "                 \"Inconnue\" : liste_unknow\n",
    "                 }\n",
    "    \n",
    "    nb_true = len(liste_true)\n",
    "    nb_false = len(liste_false)\n",
    "    nb_mix = len(liste_mix)\n",
    "    nb_unknow = len(liste_unknow)\n",
    "            \n",
    "    print(f\"Infos corpus {name}: {nb_true=}, {nb_false=}, {nb_mix=}, {nb_unknow=}\")\n",
    "    return dico_info\n",
    "\n",
    "def split(dataset: List[Dict[str, str]], data: List[int], path: str) -> List[Article] :\n",
    "    \"\"\"\n",
    "    Divise un dataset en un sous-ensemble en fonction d'une liste d'indices donnée, puis sauvegarde ce sous-ensemble au format JSON.\n",
    "    \n",
    "    Parameters :\n",
    "    dataset -- une liste de dictionnaires représentant les données complètes\n",
    "    data -- une liste d'entiers représentant les indices des éléments à extraire\n",
    "    path -- le chemin vers le fichier JSON de sortie\n",
    "    \n",
    "    Returns :\n",
    "    Une liste d'objets Article correspondant au sous-ensemble extrait\n",
    "    \"\"\"\n",
    "    matched = []\n",
    "    for item in data:        \n",
    "        element = dataset[item]\n",
    "        # print(element)\n",
    "        match = Article(id=element[\"id\"], \n",
    "                        url=element[\"url\"], \n",
    "                        author=element[\"author\"], \n",
    "                        date=element[\"date\"], \n",
    "                        rating=element[\"rating\"], \n",
    "                        title=element[\"title\"], \n",
    "                        resume=element[\"resume\"], \n",
    "                        content=element[\"content\"], \n",
    "                        category=element[\"category\"]\n",
    "                        )\n",
    "        matched.append(match)\n",
    "    split_corpus = Corpus(matched)\n",
    "    save_json(split_corpus, path)\n",
    "    \n",
    "    return load_json(path)\n",
    "\n",
    "def main():\n",
    "    path_corpura = \"../Data/data.json\"\n",
    "    \n",
    "    # Appel la fonction load_json\n",
    "    dataset = load_json(path_corpura)\n",
    "    \n",
    "    corpus_index = get_index(dataset, \"origin\")\n",
    "    # print(corpus_index)\n",
    "    \n",
    "    # Découpage des données en 3 set : train(80%), test(10%), dev(10%)\n",
    "    id_liste = [item[\"id\"] for item in dataset]\n",
    "    \n",
    "    # Train\n",
    "    size_train = math.ceil(len(id_liste) * 0.8)\n",
    "    print(size_train)\n",
    "    train = random.sample(id_liste, size_train)\n",
    "    corpus_train = split(dataset, train, \"../Corpus/train.json\")\n",
    "    train_index = get_index(corpus_train, \"train\")\n",
    "\n",
    "    # Test\n",
    "    remain = list(set(id_liste).difference(train))\n",
    "    test = random.sample(remain, math.ceil((len(id_liste) - len(train)) / 2))\n",
    "    corpus_test = split(dataset, test, \"../Corpus/test.json\")\n",
    "    test_index = get_index(corpus_test, \"test\")\n",
    "    \n",
    "    # Dev\n",
    "    dev = list(set(remain).difference(test))\n",
    "    corpus_dev = split(dataset, dev, \"../Corpus/dev.json\")\n",
    "    dev_index = get_index(corpus_dev, \"dev\")\n",
    "    \n",
    "    # On recupère le contenu textuel de chaque set\n",
    "    train_txt = corpus_train[\"content\"]\n",
    "    test_txt = corpus_test[\"content\"]\n",
    "    dev_txt = corpus_dev[\"content\"]\n",
    "    \n",
    "    # On vectorise \n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train = vectorizer.fit_transform(train_txt) # On utilise la methode .fit_tranform() pour le train\n",
    "    X_test = vectorizer.transform(test_txt) # On utilise la methode .tranform() pour le test\n",
    "    X_dev =  vectorizer.transform(dev_txt)\n",
    "    \n",
    "    # On recupère le label \"rating\" de chaque set\n",
    "    train_labels = corpus_train[\"rating\"]\n",
    "    test_labels = corpus_test[\"rating\"]\n",
    "    dev_labels = corpus_dev[\"rating\"]\n",
    "    \n",
    "    # Entrainement du modèle sur le Train\n",
    "    clf = LinearSVC().fit(X_train, train_labels)\n",
    "\n",
    "    #Score\n",
    "    print(clf.score(X_test, test_labels))\n",
    "    \n",
    "    #Predict\n",
    "    print(\"predictions:\", clf.predict(X_test))\n",
    "    \n",
    "    #Donnee\n",
    "    print(\"vraies classes:\",test_labels)\n",
    "    \n",
    "    pred = clf.predict(X_test)\n",
    "    \n",
    "    # Visualisation des résultats\n",
    "    cm = confusion_matrix(pred, test_labels, labels=clf.classes_)\n",
    "    ConfusionMatrixDisplay(cm, display_labels=clf.classes_).plot()\n",
    "\n",
    "\n",
    "#__________MAIN\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "###_END_###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ea1f7-2263-467a-9836-18b706a8a6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
